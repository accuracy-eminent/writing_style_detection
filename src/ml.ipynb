{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gutenbergpy.textget\n",
    "import re\n",
    "import string\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
    "from plotnine import *\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in datasets\n",
    "book_contents_train = utils.load_book_contents(utils.book_authors_train)\n",
    "book_contents_test = utils.load_book_contents(utils.book_authors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "books_train_wtoks = utils.wtok_books(book_contents_train)\n",
    "books_test_wtoks = utils.wtok_books(book_contents_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 100 samples per book of around 1000 words each\n",
    "from importlib import reload\n",
    "reload(utils)\n",
    "book_samples_train = utils.get_samples(books_train_wtoks, 100, [10, 1000], random_seed=42)\n",
    "book_samples_test = utils.get_samples(books_test_wtoks, 100, [10, 1000], random_seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      cd_1grams  cd_2grams  cd_3grams  ja_1grams  ja_2grams  ja_3grams  \\\n",
      "0      0.000000   0.000000   0.000000   0.008929   0.008929   0.000000   \n",
      "1      0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "2      0.002817   0.004225   0.001408   0.011268   0.002817   0.001408   \n",
      "3      0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "4      0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "1795   0.000000   0.000000   0.000000   0.045016   0.017685   0.003215   \n",
      "1796   0.000000   0.000000   0.000000   0.022222   0.000000   0.011111   \n",
      "1797   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "1798   0.000000   0.000000   0.000000   0.034783   0.017391   0.006957   \n",
      "1799   0.000000   0.000000   0.000000   0.055034   0.030872   0.004027   \n",
      "\n",
      "      hm_1grams  hm_2grams  hm_3grams  book_id  sample_num  \n",
      "0      0.000000   0.000000        0.0     46.0         0.0  \n",
      "1      0.000000   0.000000        0.0     46.0         1.0  \n",
      "2      0.000000   0.001408        0.0     46.0         2.0  \n",
      "3      0.000000   0.000000        0.0     46.0         3.0  \n",
      "4      0.002941   0.000000        0.0     46.0         4.0  \n",
      "...         ...        ...        ...      ...         ...  \n",
      "1795   0.000000   0.000000        0.0    141.0        95.0  \n",
      "1796   0.000000   0.000000        0.0    141.0        96.0  \n",
      "1797   0.000000   0.000000        0.0    141.0        97.0  \n",
      "1798   0.001739   0.000000        0.0    141.0        98.0  \n",
      "1799   0.000000   0.000000        0.0    141.0        99.0  \n",
      "\n",
      "[1800 rows x 11 columns]\n",
      "        author_name  book_id\n",
      "0   Charles Dickens       46\n",
      "1   Charles Dickens       98\n",
      "2   Charles Dickens     1400\n",
      "3   Charles Dickens      730\n",
      "4   Charles Dickens      766\n",
      "5   Charles Dickens     1023\n",
      "6   Herman Melville     2701\n",
      "7   Herman Melville    11231\n",
      "8   Herman Melville    15859\n",
      "9   Herman Melville    21816\n",
      "10  Herman Melville    34970\n",
      "11  Herman Melville    10712\n",
      "12      Jane Austen     1342\n",
      "13      Jane Austen      158\n",
      "14      Jane Austen      161\n",
      "15      Jane Austen      105\n",
      "16      Jane Austen      121\n",
      "17      Jane Austen      141\n",
      "     cd_1grams  cd_2grams  cd_3grams  ja_1grams  ja_2grams  ja_3grams  \\\n",
      "0     0.017857   0.026786   0.008929   0.000000   0.000000   0.000000   \n",
      "1     0.007143   0.007143   0.003571   0.042857   0.014286   0.010714   \n",
      "2     0.004225   0.007042   0.001408   0.018310   0.002817   0.000000   \n",
      "3     0.015267   0.015267   0.007634   0.038168   0.000000   0.000000   \n",
      "4     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "..         ...        ...        ...        ...        ...        ...   \n",
      "995   0.000000   0.000000   0.000000   0.024116   0.004823   0.000000   \n",
      "996   0.000000   0.000000   0.000000   0.022222   0.000000   0.011111   \n",
      "997   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "998   0.001739   0.001739   0.000000   0.006957   0.000000   0.001739   \n",
      "999   0.001342   0.001342   0.000000   0.017450   0.004027   0.004027   \n",
      "\n",
      "     hm_1grams  hm_2grams  hm_3grams  book_id  sample_num  \n",
      "0     0.000000   0.000000        0.0    786.0         0.0  \n",
      "1     0.000000   0.003571        0.0    786.0         1.0  \n",
      "2     0.001408   0.000000        0.0    786.0         2.0  \n",
      "3     0.000000   0.000000        0.0    786.0         3.0  \n",
      "4     0.005882   0.002941        0.0    786.0         4.0  \n",
      "..         ...        ...        ...      ...         ...  \n",
      "995   0.001608   0.000000        0.0   1212.0        95.0  \n",
      "996   0.000000   0.000000        0.0   1212.0        96.0  \n",
      "997   0.000000   0.000000        0.0   1212.0        97.0  \n",
      "998   0.000000   0.000000        0.0   1212.0        98.0  \n",
      "999   0.001342   0.002685        0.0   1212.0        99.0  \n",
      "\n",
      "[1000 rows x 11 columns]\n",
      "        author_name  book_id\n",
      "0   Charles Dickens    786.0\n",
      "1   Charles Dickens    580.0\n",
      "2   Charles Dickens    883.0\n",
      "3   Charles Dickens      NaN\n",
      "4   Charles Dickens      NaN\n",
      "5   Herman Melville   4045.0\n",
      "6   Herman Melville   8118.0\n",
      "7   Herman Melville   2694.0\n",
      "8   Herman Melville  13720.0\n",
      "9   Herman Melville  53861.0\n",
      "10      Jane Austen    946.0\n",
      "11      Jane Austen   1212.0\n",
      "12      Jane Austen      NaN\n",
      "13      Jane Austen      NaN\n",
      "14      Jane Austen      NaN\n"
     ]
    }
   ],
   "source": [
    "# Do feature engineering\n",
    "# Use ngram frequency as features\n",
    "# cd_1grams is the frequency of 1-grams associated with Charles Dickens, for example\n",
    "data_df_train = utils.get_data_df(book_samples_train, utils.book_authors_train)\n",
    "data_df_test = utils.get_data_df(book_samples_test, utils.book_authors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_cols = data_df_test.columns\n",
    "tgt_cols = ['author_name']\n",
    "X_train = data_df_train.drop(tgt_cols,axis=1)\n",
    "y_train = data_df_train.filter(tgt_cols).to_numpy().ravel()\n",
    "X_test = data_df_test.drop(tgt_cols,axis=1)\n",
    "y_test = data_df_test.filter(tgt_cols).to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 9)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cd_1grams',\n",
       " 'cd_2grams',\n",
       " 'cd_3grams',\n",
       " 'ja_1grams',\n",
       " 'ja_2grams',\n",
       " 'ja_3grams',\n",
       " 'hm_1grams',\n",
       " 'hm_2grams',\n",
       " 'hm_3grams']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.742\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Charles Dickens</th>\n",
       "      <th>Jane Austen</th>\n",
       "      <th>Herman Melville</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Charles Dickens</th>\n",
       "      <td>218</td>\n",
       "      <td>47</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jane Austen</th>\n",
       "      <td>97</td>\n",
       "      <td>371</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Herman Melville</th>\n",
       "      <td>13</td>\n",
       "      <td>34</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Charles Dickens  Jane Austen  Herman Melville\n",
       "Charles Dickens              218           47               35\n",
       "Jane Austen                   97          371               32\n",
       "Herman Melville               13           34              153"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Charles Dickens       0.66      0.73      0.69       300\n",
      "Herman Melville       0.82      0.74      0.78       500\n",
      "    Jane Austen       0.70      0.77      0.73       200\n",
      "\n",
      "       accuracy                           0.74      1000\n",
      "      macro avg       0.73      0.74      0.73      1000\n",
      "   weighted avg       0.75      0.74      0.74      1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Charles Dickens</th>\n",
       "      <th>Jane Austen</th>\n",
       "      <th>Herman Melville</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Charles Dickens</th>\n",
       "      <td>218</td>\n",
       "      <td>47</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jane Austen</th>\n",
       "      <td>97</td>\n",
       "      <td>371</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Herman Melville</th>\n",
       "      <td>13</td>\n",
       "      <td>34</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Charles Dickens  Jane Austen  Herman Melville\n",
       "Charles Dickens              218           47               35\n",
       "Jane Austen                   97          371               32\n",
       "Herman Melville               13           34              153"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_model =  GradientBoostingClassifier()\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred = gb_model.predict(X_test)\n",
    "acc = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", acc)\n",
    "conf_matrix = pd.DataFrame(metrics.confusion_matrix(y_test, y_pred))\n",
    "conf_matrix.index = set(y_train)\n",
    "conf_matrix.columns = conf_matrix.index\n",
    "display(conf_matrix)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72222222, 0.76388889, 0.81388889, 0.78055556, 0.79722222])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores = cross_val_score(gb_model, X_train, y_train, cv=5)\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ja_1grams</td>\n",
       "      <td>0.302603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cd_1grams</td>\n",
       "      <td>0.204245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ja_2grams</td>\n",
       "      <td>0.110881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ja_3grams</td>\n",
       "      <td>0.110865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hm_1grams</td>\n",
       "      <td>0.087898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cd_2grams</td>\n",
       "      <td>0.086048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hm_2grams</td>\n",
       "      <td>0.053378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cd_3grams</td>\n",
       "      <td>0.022582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hm_3grams</td>\n",
       "      <td>0.021499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature Importance\n",
       "3  ja_1grams   0.302603\n",
       "0  cd_1grams   0.204245\n",
       "4  ja_2grams   0.110881\n",
       "5  ja_3grams   0.110865\n",
       "6  hm_1grams   0.087898\n",
       "1  cd_2grams   0.086048\n",
       "7  hm_2grams   0.053378\n",
       "2  cd_3grams   0.022582\n",
       "8  hm_3grams   0.021499"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame([X_train.columns, gb_model.feature_importances_]).T\n",
    "feature_importances.columns = ['Feature','Importance']\n",
    "feature_importances.sort_values('Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Charles Dickens</th>\n",
       "      <th>Herman Melville</th>\n",
       "      <th>Jane Austen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>786.0</td>\n",
       "      <td>4045</td>\n",
       "      <td>946.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>580.0</td>\n",
       "      <td>8118</td>\n",
       "      <td>1212.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>883.0</td>\n",
       "      <td>2694</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13720</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>53861</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Charles Dickens  Herman Melville  Jane Austen\n",
       "0            786.0             4045        946.0\n",
       "1            580.0             8118       1212.0\n",
       "2            883.0             2694          NaN\n",
       "3              NaN            13720          NaN\n",
       "4              NaN            53861          NaN"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dict([(k, pd.Series(v)) for k, v in utils.book_authors_test.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_num(data, sample_id):\n",
    "    sample_id = int(str(sample_id).split(\"_\")[0])\n",
    "\n",
    "    authors_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in data.items()]))\n",
    "    author_name = pd.melt(authors_df).query(\"value == @sample_id\").variable.tolist()[0]\n",
    "    return ['Charles Dickens', 'Jane Austen','Herman Melville'].index(author_name)\n",
    "def stretch(arr, min_len, pad='<PAD>'):\n",
    "    return arr + (min_len-len(arr))* [pad]\n",
    "#train_data = [(stretch(words, 1000, '<PAD>'), get_author_num(utils.book_authors_train, sample_id)) for sample_id, words in book_samples_train.items()]\n",
    "#test_data = [(stretch(words, 1000, '<PAD>'), get_author_num(utils.book_authors_test, sample_id)) for sample_id, words in book_samples_test.items()]\n",
    "train_data = [(words, get_author_num(utils.book_authors_train, sample_id)) for sample_id, words in book_samples_train.items()]\n",
    "test_data = [(words, get_author_num(utils.book_authors_test, sample_id)) for sample_id, words in book_samples_test.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pencil-case', ',', 'a', 'pair', 'of', 'sleeve-buttons', ',', 'and', 'a']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0][-10:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_words = [word for words, _ in train_data for word in words]\n",
    "word_counts = Counter(all_words)\n",
    "vocab = {word: idx + 1 for idx, (word, count) in enumerate(word_counts.most_common(1000))}\n",
    "\n",
    "vocab['<PAD>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_df = (\n",
    "    pd.DataFrame({k: pd.Series(v) for k, v in vocab.items()})\n",
    "    .T.reset_index()\n",
    ")\n",
    "vocab_df.columns = ['name', 'num']\n",
    "vocab_df.to_csv(\"../data/vocab.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',': 1,\n",
       " 'the': 2,\n",
       " '.': 3,\n",
       " 'and': 4,\n",
       " 'of': 5,\n",
       " 'to': 6,\n",
       " 'a': 7,\n",
       " 'I': 8,\n",
       " 'in': 9,\n",
       " ';': 10,\n",
       " 'was': 11,\n",
       " 'that': 12,\n",
       " '“': 13,\n",
       " '”': 14,\n",
       " 'it': 15,\n",
       " 'his': 16,\n",
       " 'he': 17,\n",
       " '’': 18,\n",
       " 'her': 19,\n",
       " 'not': 20,\n",
       " 'with': 21,\n",
       " 'as': 22,\n",
       " 'had': 23,\n",
       " 'you': 24,\n",
       " 'for': 25,\n",
       " 'be': 26,\n",
       " 'at': 27,\n",
       " 'is': 28,\n",
       " '!': 29,\n",
       " 'have': 30,\n",
       " 'him': 31,\n",
       " 'my': 32,\n",
       " 'on': 33,\n",
       " 'she': 34,\n",
       " '?': 35,\n",
       " 'but': 36,\n",
       " 'by': 37,\n",
       " 'all': 38,\n",
       " 's': 39,\n",
       " 'me': 40,\n",
       " 'so': 41,\n",
       " 'from': 42,\n",
       " 'which': 43,\n",
       " 'this': 44,\n",
       " 'said': 45,\n",
       " 'were': 46,\n",
       " 'would': 47,\n",
       " 'been': 48,\n",
       " 'or': 49,\n",
       " 'one': 50,\n",
       " \"''\": 51,\n",
       " 'no': 52,\n",
       " 'they': 53,\n",
       " 'an': 54,\n",
       " 'very': 55,\n",
       " 'The': 56,\n",
       " 'could': 57,\n",
       " 'Mr.': 58,\n",
       " '``': 59,\n",
       " 'there': 60,\n",
       " 'them': 61,\n",
       " 'are': 62,\n",
       " 'when': 63,\n",
       " 'their': 64,\n",
       " 'what': 65,\n",
       " 'if': 66,\n",
       " 'more': 67,\n",
       " 'do': 68,\n",
       " 'But': 69,\n",
       " '--': 70,\n",
       " 'any': 71,\n",
       " 'will': 72,\n",
       " 'some': 73,\n",
       " 'out': 74,\n",
       " 'now': 75,\n",
       " 'upon': 76,\n",
       " 'who': 77,\n",
       " 'your': 78,\n",
       " 'It': 79,\n",
       " 'than': 80,\n",
       " 'into': 81,\n",
       " 'He': 82,\n",
       " 'such': 83,\n",
       " 'man': 84,\n",
       " 'little': 85,\n",
       " 'up': 86,\n",
       " 'must': 87,\n",
       " 'can': 88,\n",
       " 'time': 89,\n",
       " 'we': 90,\n",
       " 'being': 91,\n",
       " 'much': 92,\n",
       " 'should': 93,\n",
       " 'other': 94,\n",
       " 'know': 95,\n",
       " 'am': 96,\n",
       " 'like': 97,\n",
       " 'own': 98,\n",
       " ':': 99,\n",
       " 'only': 100,\n",
       " 'before': 101,\n",
       " 'never': 102,\n",
       " 'did': 103,\n",
       " 'about': 104,\n",
       " 'then': 105,\n",
       " 'Mrs.': 106,\n",
       " 'its': 107,\n",
       " 'has': 108,\n",
       " 'She': 109,\n",
       " 'might': 110,\n",
       " 'And': 111,\n",
       " 'good': 112,\n",
       " 'again': 113,\n",
       " 'old': 114,\n",
       " 'think': 115,\n",
       " '‘': 116,\n",
       " 'say': 117,\n",
       " 'Miss': 118,\n",
       " 'see': 119,\n",
       " 'though': 120,\n",
       " 'every': 121,\n",
       " 'down': 122,\n",
       " 'made': 123,\n",
       " 'may': 124,\n",
       " 'You': 125,\n",
       " 'over': 126,\n",
       " 'too': 127,\n",
       " 'two': 128,\n",
       " 'first': 129,\n",
       " 'great': 130,\n",
       " 'himself': 131,\n",
       " 'how': 132,\n",
       " 't': 133,\n",
       " 'most': 134,\n",
       " 'after': 135,\n",
       " 'without': 136,\n",
       " 'well': 137,\n",
       " 'way': 138,\n",
       " 'nothing': 139,\n",
       " 'long': 140,\n",
       " 'here': 141,\n",
       " 'day': 142,\n",
       " 'thought': 143,\n",
       " 'last': 144,\n",
       " 'come': 145,\n",
       " 'these': 146,\n",
       " 'away': 147,\n",
       " 'young': 148,\n",
       " \"'s\": 149,\n",
       " 'us': 150,\n",
       " 'go': 151,\n",
       " 'came': 152,\n",
       " 'where': 153,\n",
       " 'yet': 154,\n",
       " 'still': 155,\n",
       " 'seemed': 156,\n",
       " 'our': 157,\n",
       " 'shall': 158,\n",
       " 'ever': 159,\n",
       " 'thing': 160,\n",
       " 'quite': 161,\n",
       " 'while': 162,\n",
       " 'In': 163,\n",
       " 'hand': 164,\n",
       " 'make': 165,\n",
       " 'dear': 166,\n",
       " 'A': 167,\n",
       " 'those': 168,\n",
       " 'herself': 169,\n",
       " 'always': 170,\n",
       " 'What': 171,\n",
       " 'through': 172,\n",
       " 'many': 173,\n",
       " 'even': 174,\n",
       " 'head': 175,\n",
       " 'another': 176,\n",
       " ')': 177,\n",
       " 'room': 178,\n",
       " '(': 179,\n",
       " 'mind': 180,\n",
       " 'There': 181,\n",
       " 'back': 182,\n",
       " 'sir': 183,\n",
       " 'soon': 184,\n",
       " 'same': 185,\n",
       " 'Scrooge': 186,\n",
       " 'looked': 187,\n",
       " 'house': 188,\n",
       " 'off': 189,\n",
       " 'look': 190,\n",
       " 'face': 191,\n",
       " 'went': 192,\n",
       " 'place': 193,\n",
       " 'saw': 194,\n",
       " 'They': 195,\n",
       " 'take': 196,\n",
       " 'having': 197,\n",
       " 'myself': 198,\n",
       " 'done': 199,\n",
       " 'moment': 200,\n",
       " 'eyes': 201,\n",
       " 'No': 202,\n",
       " 'night': 203,\n",
       " 'better': 204,\n",
       " 'life': 205,\n",
       " 'something': 206,\n",
       " 'just': 207,\n",
       " 'once': 208,\n",
       " 'going': 209,\n",
       " 'My': 210,\n",
       " 'If': 211,\n",
       " 'friend': 212,\n",
       " 'morning': 213,\n",
       " 'Bartleby': 214,\n",
       " 'left': 215,\n",
       " 'round': 216,\n",
       " 'heart': 217,\n",
       " 'This': 218,\n",
       " 'Captain': 219,\n",
       " 'cried': 220,\n",
       " 'give': 221,\n",
       " 'found': 222,\n",
       " 'Pierre': 223,\n",
       " 'mother': 224,\n",
       " 'door': 225,\n",
       " 'seen': 226,\n",
       " 'As': 227,\n",
       " 'father': 228,\n",
       " 'felt': 229,\n",
       " 'three': 230,\n",
       " 'replied': 231,\n",
       " 'part': 232,\n",
       " 'under': 233,\n",
       " 'home': 234,\n",
       " 'sister': 235,\n",
       " 'indeed': 236,\n",
       " 'enough': 237,\n",
       " 'put': 238,\n",
       " 'between': 239,\n",
       " 'heard': 240,\n",
       " 'far': 241,\n",
       " 'men': 242,\n",
       " 'poor': 243,\n",
       " 'against': 244,\n",
       " 'towards': 245,\n",
       " 'few': 246,\n",
       " 'almost': 247,\n",
       " 'hands': 248,\n",
       " 'however': 249,\n",
       " 'knew': 250,\n",
       " 'hope': 251,\n",
       " 'rather': 252,\n",
       " 'among': 253,\n",
       " 'present': 254,\n",
       " 'side': 255,\n",
       " 'tell': 256,\n",
       " 'sure': 257,\n",
       " 'world': 258,\n",
       " 'Oh': 259,\n",
       " 'At': 260,\n",
       " 'things': 261,\n",
       " 'gentleman': 262,\n",
       " 'less': 263,\n",
       " 'whole': 264,\n",
       " 'business': 265,\n",
       " 'When': 266,\n",
       " 'whom': 267,\n",
       " 'anything': 268,\n",
       " 'let': 269,\n",
       " 'looking': 270,\n",
       " 'both': 271,\n",
       " 'Sir': 272,\n",
       " 'half': 273,\n",
       " 'people': 274,\n",
       " 'That': 275,\n",
       " 'love': 276,\n",
       " 'woman': 277,\n",
       " 'called': 278,\n",
       " 'returned': 279,\n",
       " 'Catherine': 280,\n",
       " 'Yes': 281,\n",
       " 'next': 282,\n",
       " 'We': 283,\n",
       " 'word': 284,\n",
       " 'least': 285,\n",
       " 'right': 286,\n",
       " 'told': 287,\n",
       " 'For': 288,\n",
       " 'really': 289,\n",
       " 'together': 290,\n",
       " 'How': 291,\n",
       " 'years': 292,\n",
       " 'sort': 293,\n",
       " 'manner': 294,\n",
       " 'So': 295,\n",
       " 'Now': 296,\n",
       " 'ship': 297,\n",
       " 'His': 298,\n",
       " 'took': 299,\n",
       " 'family': 300,\n",
       " 'turned': 301,\n",
       " 'believe': 302,\n",
       " 'perhaps': 303,\n",
       " 'till': 304,\n",
       " 'gone': 305,\n",
       " 'air': 306,\n",
       " 'find': 307,\n",
       " 'light': 308,\n",
       " 'hear': 309,\n",
       " 'don': 310,\n",
       " 'each': 311,\n",
       " 'nor': 312,\n",
       " 'stood': 313,\n",
       " 'boy': 314,\n",
       " 'sat': 315,\n",
       " 'speak': 316,\n",
       " 'does': 317,\n",
       " 'happy': 318,\n",
       " 'short': 319,\n",
       " 'brought': 320,\n",
       " 'brother': 321,\n",
       " 'kind': 322,\n",
       " 'voice': 323,\n",
       " 'days': 324,\n",
       " \"n't\": 325,\n",
       " 'passed': 326,\n",
       " 'because': 327,\n",
       " 'best': 328,\n",
       " 'whether': 329,\n",
       " 'To': 330,\n",
       " 'words': 331,\n",
       " 'Lady': 332,\n",
       " 'certain': 333,\n",
       " 'began': 334,\n",
       " 'small': 335,\n",
       " 'Fanny': 336,\n",
       " 'Well': 337,\n",
       " 'get': 338,\n",
       " 'Elizabeth': 339,\n",
       " 'course': 340,\n",
       " 'got': 341,\n",
       " 'Oliver': 342,\n",
       " 'name': 343,\n",
       " 'person': 344,\n",
       " 'case': 345,\n",
       " 'lady': 346,\n",
       " 'often': 347,\n",
       " 'known': 348,\n",
       " 'wish': 349,\n",
       " 'Why': 350,\n",
       " 'new': 351,\n",
       " 'hour': 352,\n",
       " 'feel': 353,\n",
       " 'times': 354,\n",
       " 'Not': 355,\n",
       " 'asked': 356,\n",
       " 'set': 357,\n",
       " 'CHAPTER': 358,\n",
       " 'taken': 359,\n",
       " 'table': 360,\n",
       " 'coming': 361,\n",
       " 'within': 362,\n",
       " 'whose': 363,\n",
       " 'letter': 364,\n",
       " 'child': 365,\n",
       " 'evening': 366,\n",
       " 'else': 367,\n",
       " 'girl': 368,\n",
       " 'end': 369,\n",
       " 'keep': 370,\n",
       " 'Anne': 371,\n",
       " 'hardly': 372,\n",
       " 'given': 373,\n",
       " 'gave': 374,\n",
       " 'leave': 375,\n",
       " 'feelings': 376,\n",
       " 'point': 377,\n",
       " 'since': 378,\n",
       " 'eye': 379,\n",
       " 'Her': 380,\n",
       " 'rest': 381,\n",
       " 'others': 382,\n",
       " 'Elinor': 383,\n",
       " 'open': 384,\n",
       " 'behind': 385,\n",
       " 'want': 386,\n",
       " 'full': 387,\n",
       " 'near': 388,\n",
       " 'aunt': 389,\n",
       " 'taking': 390,\n",
       " 'strange': 391,\n",
       " 'Then': 392,\n",
       " 'making': 393,\n",
       " 'reason': 394,\n",
       " 'general': 395,\n",
       " 'Emma': 396,\n",
       " 'answer': 397,\n",
       " 'fire': 398,\n",
       " 'call': 399,\n",
       " 'friends': 400,\n",
       " 'subject': 401,\n",
       " 'matter': 402,\n",
       " 'themselves': 403,\n",
       " 'help': 404,\n",
       " 'possible': 405,\n",
       " 'alone': 406,\n",
       " 'thus': 407,\n",
       " 'Mrs': 408,\n",
       " 'pleasure': 409,\n",
       " 'office': 410,\n",
       " 'either': 411,\n",
       " 'suppose': 412,\n",
       " 'large': 413,\n",
       " 'read': 414,\n",
       " 'With': 415,\n",
       " 'sea': 416,\n",
       " 'idea': 417,\n",
       " 'Do': 418,\n",
       " 'Mr': 419,\n",
       " 'sight': 420,\n",
       " 'character': 421,\n",
       " 'doubt': 422,\n",
       " 'mean': 423,\n",
       " 'immediately': 424,\n",
       " 'says': 425,\n",
       " 'fine': 426,\n",
       " 'opinion': 427,\n",
       " 'account': 428,\n",
       " 'certainly': 429,\n",
       " 'means': 430,\n",
       " 'll': 431,\n",
       " 'Marianne': 432,\n",
       " 'work': 433,\n",
       " 'high': 434,\n",
       " 'body': 435,\n",
       " 'walked': 436,\n",
       " 'black': 437,\n",
       " 'spoke': 438,\n",
       " 'itself': 439,\n",
       " 'yourself': 440,\n",
       " 'therefore': 441,\n",
       " 'true': 442,\n",
       " 'why': 443,\n",
       " 'four': 444,\n",
       " 'Lucy': 445,\n",
       " 'window': 446,\n",
       " 'above': 447,\n",
       " 'bed': 448,\n",
       " 'dark': 449,\n",
       " 'Joe': 450,\n",
       " 'appeared': 451,\n",
       " 'Jane': 452,\n",
       " 'common': 453,\n",
       " 'fellow': 454,\n",
       " 'sometimes': 455,\n",
       " 'strong': 456,\n",
       " 'seeing': 457,\n",
       " 'state': 458,\n",
       " 'white': 459,\n",
       " 'glad': 460,\n",
       " 'nature': 461,\n",
       " 'added': 462,\n",
       " 'thou': 463,\n",
       " 'saying': 464,\n",
       " 'pretty': 465,\n",
       " 'hours': 466,\n",
       " 'spirits': 467,\n",
       " 'change': 468,\n",
       " 'entirely': 469,\n",
       " 'attention': 470,\n",
       " 'happiness': 471,\n",
       " 'beyond': 472,\n",
       " 'return': 473,\n",
       " 'usual': 474,\n",
       " 'lay': 475,\n",
       " 'daughter': 476,\n",
       " 'children': 477,\n",
       " 'Turkey': 478,\n",
       " 'talk': 479,\n",
       " 'question': 480,\n",
       " 'All': 481,\n",
       " 'sorry': 482,\n",
       " 'walk': 483,\n",
       " 'ye': 484,\n",
       " 'Christmas': 485,\n",
       " 'perfectly': 486,\n",
       " 'everything': 487,\n",
       " 'On': 488,\n",
       " 'feeling': 489,\n",
       " 'hard': 490,\n",
       " 'live': 491,\n",
       " 'along': 492,\n",
       " 'cold': 493,\n",
       " 'seem': 494,\n",
       " 'thee': 495,\n",
       " 'party': 496,\n",
       " 'wife': 497,\n",
       " 'also': 498,\n",
       " 'understand': 499,\n",
       " 'until': 500,\n",
       " 'truth': 501,\n",
       " 'view': 502,\n",
       " 'bad': 503,\n",
       " 'used': 504,\n",
       " 'standing': 505,\n",
       " 'town': 506,\n",
       " 'low': 507,\n",
       " 'already': 508,\n",
       " 'Don': 509,\n",
       " 'Crawford': 510,\n",
       " 'One': 511,\n",
       " 'turn': 512,\n",
       " 'wanted': 513,\n",
       " 'kept': 514,\n",
       " 'interest': 515,\n",
       " 'Is': 516,\n",
       " 'thinking': 517,\n",
       " 'turning': 518,\n",
       " 'different': 519,\n",
       " 'ought': 520,\n",
       " 'water': 521,\n",
       " 'received': 522,\n",
       " 'power': 523,\n",
       " 'fact': 524,\n",
       " 'answered': 525,\n",
       " 'Here': 526,\n",
       " 'ready': 527,\n",
       " \"'\": 528,\n",
       " 'soul': 529,\n",
       " 'God': 530,\n",
       " 'arm': 531,\n",
       " 'dinner': 532,\n",
       " 'entered': 533,\n",
       " 'sense': 534,\n",
       " 'company': 535,\n",
       " 'held': 536,\n",
       " 'dead': 537,\n",
       " 'whale': 538,\n",
       " 'observed': 539,\n",
       " 'mine': 540,\n",
       " 'particular': 541,\n",
       " 'purpose': 542,\n",
       " 'country': 543,\n",
       " 'quiet': 544,\n",
       " 'ask': 545,\n",
       " 'acquaintance': 546,\n",
       " 'able': 547,\n",
       " 'comfort': 548,\n",
       " 'stay': 549,\n",
       " 'doing': 550,\n",
       " '*': 551,\n",
       " 'close': 552,\n",
       " 'appearance': 553,\n",
       " 'longer': 554,\n",
       " 'continued': 555,\n",
       " 'none': 556,\n",
       " 'Ah': 557,\n",
       " 'minutes': 558,\n",
       " 'ladies': 559,\n",
       " 'became': 560,\n",
       " 'son': 561,\n",
       " 'Let': 562,\n",
       " 'deal': 563,\n",
       " 'circumstances': 564,\n",
       " 'ten': 565,\n",
       " 'ground': 566,\n",
       " 'thoughts': 567,\n",
       " 'Tilney': 568,\n",
       " 'fell': 569,\n",
       " 'London': 570,\n",
       " 'Darcy': 571,\n",
       " 'Ghost': 572,\n",
       " 'object': 573,\n",
       " 'impossible': 574,\n",
       " 'sound': 575,\n",
       " 'late': 576,\n",
       " 'occasion': 577,\n",
       " 'After': 578,\n",
       " 'lost': 579,\n",
       " 'further': 580,\n",
       " 'length': 581,\n",
       " 'bear': 582,\n",
       " 'hold': 583,\n",
       " 'Nippers': 584,\n",
       " 'Elliot': 585,\n",
       " 'money': 586,\n",
       " 'chair': 587,\n",
       " 'Harriet': 588,\n",
       " 'bring': 589,\n",
       " 'use': 590,\n",
       " 'comes': 591,\n",
       " 'conversation': 592,\n",
       " 'five': 593,\n",
       " 'obliged': 594,\n",
       " 'wall': 595,\n",
       " 'dare': 596,\n",
       " 'past': 597,\n",
       " 'second': 598,\n",
       " 'forward': 599,\n",
       " 'afterwards': 600,\n",
       " 'remained': 601,\n",
       " 'care': 602,\n",
       " 'stopped': 603,\n",
       " 'feet': 604,\n",
       " 'wished': 605,\n",
       " 'confidence': 606,\n",
       " 'natural': 607,\n",
       " 'Bennet': 608,\n",
       " 'stand': 609,\n",
       " 'week': 610,\n",
       " 'instant': 611,\n",
       " 'reply': 612,\n",
       " 'Doctor': 613,\n",
       " 'prefer': 614,\n",
       " 'show': 615,\n",
       " 'death': 616,\n",
       " 'Bumble': 617,\n",
       " 'John': 618,\n",
       " 'opened': 619,\n",
       " 'clock': 620,\n",
       " 'appear': 621,\n",
       " 'consider': 622,\n",
       " 'Charles': 623,\n",
       " 'afraid': 624,\n",
       " 'tried': 625,\n",
       " 'corner': 626,\n",
       " 'talked': 627,\n",
       " 'Lorry': 628,\n",
       " 'suddenly': 629,\n",
       " 'seems': 630,\n",
       " 'human': 631,\n",
       " 'Your': 632,\n",
       " 'arms': 633,\n",
       " 'tone': 634,\n",
       " 'affection': 635,\n",
       " 'degree': 636,\n",
       " 'engaged': 637,\n",
       " 'boat': 638,\n",
       " 'Bath': 639,\n",
       " 'creature': 640,\n",
       " 'followed': 641,\n",
       " 'Come': 642,\n",
       " 'happened': 643,\n",
       " 'effect': 644,\n",
       " 'visit': 645,\n",
       " 'year': 646,\n",
       " 'living': 647,\n",
       " 'wonder': 648,\n",
       " 'master': 649,\n",
       " 'expression': 650,\n",
       " 'speaking': 651,\n",
       " 'boys': 652,\n",
       " 'remember': 653,\n",
       " 'stranger': 654,\n",
       " 'sudden': 655,\n",
       " 'Wentworth': 656,\n",
       " 'former': 657,\n",
       " 'neither': 658,\n",
       " 'giving': 659,\n",
       " 'cause': 660,\n",
       " 'real': 661,\n",
       " 'Isabel': 662,\n",
       " 'mere': 663,\n",
       " 'especially': 664,\n",
       " 'supposed': 665,\n",
       " 'board': 666,\n",
       " 'except': 667,\n",
       " 'hundred': 668,\n",
       " 'book': 669,\n",
       " 'Spirit': 670,\n",
       " 'sake': 671,\n",
       " 'silence': 672,\n",
       " 'carriage': 673,\n",
       " 'whatever': 674,\n",
       " 'during': 675,\n",
       " 'Weston': 676,\n",
       " 'agreeable': 677,\n",
       " 'need': 678,\n",
       " 'it.': 679,\n",
       " 'Isabella': 680,\n",
       " 'met': 681,\n",
       " 'pleasant': 682,\n",
       " 'spirit': 683,\n",
       " 'struck': 684,\n",
       " 'hair': 685,\n",
       " 'exactly': 686,\n",
       " 'THE': 687,\n",
       " 'story': 688,\n",
       " 'Dashwood': 689,\n",
       " 'deep': 690,\n",
       " 'likely': 691,\n",
       " 'countenance': 692,\n",
       " 'paper': 693,\n",
       " 'considered': 694,\n",
       " 'situation': 695,\n",
       " 'By': 696,\n",
       " 'afternoon': 697,\n",
       " 'sweet': 698,\n",
       " 'instead': 699,\n",
       " 'surprise': 700,\n",
       " 'gentlemen': 701,\n",
       " 'respect': 702,\n",
       " 'greater': 703,\n",
       " 'knowledge': 704,\n",
       " 'kindness': 705,\n",
       " 'ill': 706,\n",
       " 'fear': 707,\n",
       " 'calling': 708,\n",
       " 'scarcely': 709,\n",
       " 'smile': 710,\n",
       " 'meant': 711,\n",
       " 'ago': 712,\n",
       " 'led': 713,\n",
       " 'chance': 714,\n",
       " 'thousand': 715,\n",
       " 'Some': 716,\n",
       " 'expected': 717,\n",
       " 'fortune': 718,\n",
       " 'order': 719,\n",
       " 'determined': 720,\n",
       " 'figure': 721,\n",
       " 'silent': 722,\n",
       " 'married': 723,\n",
       " 'Who': 724,\n",
       " 'early': 725,\n",
       " 'age': 726,\n",
       " 'regard': 727,\n",
       " 'beginning': 728,\n",
       " 'wind': 729,\n",
       " 'husband': 730,\n",
       " 'These': 731,\n",
       " 'several': 732,\n",
       " 'sent': 733,\n",
       " 'o': 734,\n",
       " 'Edward': 735,\n",
       " 'Knightley': 736,\n",
       " 'pleased': 737,\n",
       " 'mouth': 738,\n",
       " 'street': 739,\n",
       " 'forth': 740,\n",
       " 'Edmund': 741,\n",
       " 'secret': 742,\n",
       " 'looks': 743,\n",
       " 'necessary': 744,\n",
       " 'uncle': 745,\n",
       " 'thy': 746,\n",
       " 'peculiar': 747,\n",
       " 'remain': 748,\n",
       " 'handsome': 749,\n",
       " 'heavy': 750,\n",
       " 'fair': 751,\n",
       " 'run': 752,\n",
       " 'Pip': 753,\n",
       " 'deck': 754,\n",
       " 'honour': 755,\n",
       " 'walking': 756,\n",
       " 'Micawber': 757,\n",
       " 'Henry': 758,\n",
       " 'knows': 759,\n",
       " 'getting': 760,\n",
       " 'floor': 761,\n",
       " 'knowing': 762,\n",
       " 'following': 763,\n",
       " 'Such': 764,\n",
       " 'settled': 765,\n",
       " 'form': 766,\n",
       " 'meet': 767,\n",
       " 'Of': 768,\n",
       " 'warm': 769,\n",
       " 'sitting': 770,\n",
       " 'conduct': 771,\n",
       " 'sit': 772,\n",
       " 'notice': 773,\n",
       " 'Colonel': 774,\n",
       " 'servant': 775,\n",
       " 'red': 776,\n",
       " 'distance': 777,\n",
       " 'Mary': 778,\n",
       " 'nephew': 779,\n",
       " 'vain': 780,\n",
       " 'slowly': 781,\n",
       " 'six': 782,\n",
       " 'cousin': 783,\n",
       " 'fixed': 784,\n",
       " 'sisters': 785,\n",
       " 'carried': 786,\n",
       " 'resolved': 787,\n",
       " 'meaning': 788,\n",
       " 'spoken': 789,\n",
       " 'wrong': 790,\n",
       " 'Peggotty': 791,\n",
       " 'weather': 792,\n",
       " 'yes': 793,\n",
       " 'trouble': 794,\n",
       " 'sleep': 795,\n",
       " 'directly': 796,\n",
       " 'worse': 797,\n",
       " 'dance': 798,\n",
       " 'hat': 799,\n",
       " 'fond': 800,\n",
       " 'manners': 801,\n",
       " 'duty': 802,\n",
       " 'passing': 803,\n",
       " 'repeated': 804,\n",
       " 'caught': 805,\n",
       " 'leaving': 806,\n",
       " 'otherwise': 807,\n",
       " 'plain': 808,\n",
       " 'Yet': 809,\n",
       " 'Lord': 810,\n",
       " 'lived': 811,\n",
       " 'pale': 812,\n",
       " 'hearing': 813,\n",
       " 'anybody': 814,\n",
       " 'nobody': 815,\n",
       " 'expect': 816,\n",
       " 'perfect': 817,\n",
       " 'serious': 818,\n",
       " 'please': 819,\n",
       " 'earth': 820,\n",
       " 'sun': 821,\n",
       " 'become': 822,\n",
       " 'talking': 823,\n",
       " 'act': 824,\n",
       " 'glass': 825,\n",
       " 'period': 826,\n",
       " 'Woodhouse': 827,\n",
       " 'rooms': 828,\n",
       " 'Indeed': 829,\n",
       " 'laid': 830,\n",
       " 'sensible': 831,\n",
       " 'opportunity': 832,\n",
       " 'nearly': 833,\n",
       " 'wild': 834,\n",
       " 'fancy': 835,\n",
       " 'farther': 836,\n",
       " 'generally': 837,\n",
       " 'Ahab': 838,\n",
       " 'easy': 839,\n",
       " 'grave': 840,\n",
       " 'twenty': 841,\n",
       " 'closed': 842,\n",
       " 'play': 843,\n",
       " 'tears': 844,\n",
       " 'road': 845,\n",
       " 'across': 846,\n",
       " 'equal': 847,\n",
       " 'placed': 848,\n",
       " 'write': 849,\n",
       " 'you.': 850,\n",
       " 'pain': 851,\n",
       " 'equally': 852,\n",
       " 'Delano': 853,\n",
       " 'Musgrove': 854,\n",
       " 'believed': 855,\n",
       " 'women': 856,\n",
       " 'below': 857,\n",
       " 'Every': 858,\n",
       " 'writing': 859,\n",
       " 'drew': 860,\n",
       " 'hoped': 861,\n",
       " 'officers': 862,\n",
       " 'pause': 863,\n",
       " 'Havisham': 864,\n",
       " '[': 865,\n",
       " 'Elton': 866,\n",
       " 'green': 867,\n",
       " 'clear': 868,\n",
       " 'line': 869,\n",
       " 'mentioned': 870,\n",
       " 'exclaimed': 871,\n",
       " 'pride': 872,\n",
       " 'd': 873,\n",
       " 'glance': 874,\n",
       " 'altogether': 875,\n",
       " 'Bingley': 876,\n",
       " 'moved': 877,\n",
       " 'rose': 878,\n",
       " 'instantly': 879,\n",
       " 'fast': 880,\n",
       " 'companion': 881,\n",
       " 'society': 882,\n",
       " ']': 883,\n",
       " 'Willoughby': 884,\n",
       " 'satisfaction': 885,\n",
       " 'dropped': 886,\n",
       " 'delight': 887,\n",
       " 'Upon': 888,\n",
       " 'forget': 889,\n",
       " 'desk': 890,\n",
       " 'probably': 891,\n",
       " 'original': 892,\n",
       " 'mention': 893,\n",
       " 'favour': 894,\n",
       " 'assure': 895,\n",
       " 'letters': 896,\n",
       " 'pass': 897,\n",
       " 'loved': 898,\n",
       " 'horses': 899,\n",
       " 'advantage': 900,\n",
       " 'anxious': 901,\n",
       " 'public': 902,\n",
       " 'me.': 903,\n",
       " 'aware': 904,\n",
       " 'laugh': 905,\n",
       " 'finding': 906,\n",
       " 'ran': 907,\n",
       " 'occurred': 908,\n",
       " 'hurried': 909,\n",
       " 'promise': 910,\n",
       " 'months': 911,\n",
       " 'beautiful': 912,\n",
       " 'lying': 913,\n",
       " 'touched': 914,\n",
       " 'thrown': 915,\n",
       " 'besides': 916,\n",
       " 'Richard': 917,\n",
       " 'General': 918,\n",
       " 'health': 919,\n",
       " 'influence': 920,\n",
       " 'Nothing': 921,\n",
       " 'dress': 922,\n",
       " 'satisfied': 923,\n",
       " 'danger': 924,\n",
       " 'note': 925,\n",
       " 'William': 926,\n",
       " 'running': 927,\n",
       " 'private': 928,\n",
       " 'evil': 929,\n",
       " 'third': 930,\n",
       " 'spite': 931,\n",
       " 'Are': 932,\n",
       " 'meeting': 933,\n",
       " 'putting': 934,\n",
       " 'greatest': 935,\n",
       " 'spot': 936,\n",
       " 'difference': 937,\n",
       " 'Sikes': 938,\n",
       " 'Thomas': 939,\n",
       " 'streets': 940,\n",
       " 'wonderful': 941,\n",
       " 'raised': 942,\n",
       " 'goes': 943,\n",
       " 'cut': 944,\n",
       " 'trust': 945,\n",
       " 'merely': 946,\n",
       " 'law': 947,\n",
       " 'taste': 948,\n",
       " 'Walter': 949,\n",
       " 'marriage': 950,\n",
       " 'allowed': 951,\n",
       " 'stone': 952,\n",
       " 'lips': 953,\n",
       " 'Where': 954,\n",
       " 'temper': 955,\n",
       " 'wine': 956,\n",
       " 'judge': 957,\n",
       " 'highly': 958,\n",
       " 'm': 959,\n",
       " 'written': 960,\n",
       " 'Jew': 961,\n",
       " 'prove': 962,\n",
       " 'allow': 963,\n",
       " 'bright': 964,\n",
       " 'Very': 965,\n",
       " 'seat': 966,\n",
       " 'future': 967,\n",
       " 'understood': 968,\n",
       " 'various': 969,\n",
       " 'pity': 970,\n",
       " 'desire': 971,\n",
       " 'history': 972,\n",
       " 'removed': 973,\n",
       " 'Leicester': 974,\n",
       " 'consequence': 975,\n",
       " 'single': 976,\n",
       " 'died': 977,\n",
       " 'changed': 978,\n",
       " 'summer': 979,\n",
       " 'miles': 980,\n",
       " 'begin': 981,\n",
       " 'Jennings': 982,\n",
       " 'Russell': 983,\n",
       " 'inquired': 984,\n",
       " 'everybody': 985,\n",
       " 'Look': 986,\n",
       " 'wide': 987,\n",
       " 'forgotten': 988,\n",
       " 'wholly': 989,\n",
       " 'top': 990,\n",
       " 'calm': 991,\n",
       " 'shoulder': 992,\n",
       " 'service': 993,\n",
       " 'disposed': 994,\n",
       " 'personal': 995,\n",
       " 'negro': 996,\n",
       " 'Norris': 997,\n",
       " 'laughed': 998,\n",
       " 'spent': 999,\n",
       " 'reached': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, vocab):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words, label = self.data[idx]\n",
    "        word_indices = [self.vocab.get(word, 0) for word in words]\n",
    "        return torch.tensor(word_indices), torch.tensor(int(label))  # Assuming labels are integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AuthorClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes):\n",
    "        super(AuthorClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        output = self.fc(lstm_out[:, -1, :])  # Take the last LSTM output for each sequence\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "710"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 64\n",
    "hidden_size = 128\n",
    "num_classes = len(set(label for _, label in train_data))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Model, loss, optimizer\n",
    "model = AuthorClassifier(vocab_size, embedding_dim, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_dataset = TextDataset(train_data, vocab)\n",
    "test_dataset = TextDataset(test_data, vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Loss: 0.030522226322427237\n",
      "Accuracy:  99.0\n",
      "1\n",
      "Loss: 0.027134019190254276\n",
      "Accuracy:  99.11111111111111\n",
      "2\n",
      "Loss: 0.03455847775734697\n",
      "Accuracy:  99.22222222222223\n",
      "3\n",
      "Loss: 0.004879653746814358\n",
      "Accuracy:  99.94444444444444\n",
      "4\n",
      "Loss: 0.021396659104736607\n",
      "Accuracy:  99.27777777777777\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    model.train()\n",
    "    loss_vals = []\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        loss_vals.append(loss.item())\n",
    "        _, pred_labels = torch.max(outputs, 1)\n",
    "        total_correct += (pred_labels == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        optimizer.step()\n",
    "    print(\"Loss:\", np.mean(loss_vals))\n",
    "    print(\"Accuracy: \", 100 * total_correct / total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict, \"../data/weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"../data/weights.pth\")())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 78.40%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = total_correct / total_samples\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
